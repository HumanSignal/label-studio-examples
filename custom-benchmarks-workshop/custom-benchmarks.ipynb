{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09af2a6",
   "metadata": {},
   "source": [
    "# Building Custom Benchmarks with Label Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f210bf",
   "metadata": {},
   "source": [
    "### Step 0: Install PreRequisite Packages and Add Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_studio_url = \"https://app.heartex.com\" \n",
    "label_studio_api_key = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743596d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install label-studio-sdk\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eda874",
   "metadata": {},
   "source": [
    "### Step 1: Create your first Label Studio project for Rubric Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04253631",
   "metadata": {},
   "source": [
    "<a href=\"https://app.heartex.com/b/MTI1Ng==\"\n",
    "  target=\"_blank\" rel=\"noopener\" aria-label=\"Open in Label Studio\" style=\"all:unset;cursor:pointer;display:inline-flex;align-items:center;justify-content:center;border-radius:4px;border:1px solid rgb(109,135,241);padding:8px 12px;background:rgb(87 108 193);color:white;font-weight:500;font-family:sans-serif;gap:6px;transition:background 0.2s ease;\" onmouseover=\"this.style.background='rgb(97 122 218)'\" onmouseout=\"this.style.background='rgb(87 108 193)'\">\n",
    "  <svg style=\"width:20px;height:20px\" viewBox=\"0 0 26 26\" fill=\"none\"><path fill=\"#FFBAAA\" d=\"M3.5 4.5h19v18h-19z\"/><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M25.7 7.503h-7.087V5.147H7.588V2.792h11.025V.436H25.7v7.067Zm-18.112 0H5.225v10.994H2.863V7.503H.5V.436h7.088v7.067Zm0 18.061v-7.067H.5v7.067h7.088ZM25.7 18.497v7.067h-7.088v-2.356H7.588v-2.355h11.025v-2.356H25.7Zm-2.363 0V7.503h-2.363v10.994h2.363Z\" fill=\"#FF7557\"/></svg>\n",
    "  <span style=\"font-size:14px\">Open in Label Studio</span>\n",
    "  <svg style=\"width:16px;height:16px\" viewBox=\"0 0 24 24\"><path d=\"M14,3V5H17.59L7.76,14.83L9.17,16.24L19,6.41V10H21V3M19,19H5V5H12V3H5C3.89,3 3,3.9 3,5V19A2,2 0 0,0 5,21H19A2,2 0 0,0 21,19V12H19V19Z\" fill=\"white\"/></svg>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71079f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_creation_project_id = \"212780\"\n",
    "target_workspace_id = \"94519\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194effb",
   "metadata": {},
   "source": [
    "### Step 2: Create Label Studio Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk.client import LabelStudio\n",
    "\n",
    "ls = LabelStudio(\n",
    "    base_url=label_studio_url,  \n",
    "    api_key=label_studio_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd768b8c",
   "metadata": {},
   "source": [
    "### Step 3: Create a new project for evaluating against rubrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "label_config = \"\"\"<View>\n",
    "  <!-- Top context -->\n",
    "  <Header value=\"User Query\" style=\"margin-top: 0.5em;\" />\n",
    "  <Text name=\"query\" value=\"$User_Query\" />\n",
    "\n",
    "  <Header value=\"Sample AI Response\" style=\"margin-top: 1em;\" />\n",
    "  <Text name=\"response\" value=\"$AI_Response\" />\n",
    "\n",
    "  <!-- Criteria sections -->\n",
    "  <Collapse accordion=\"false\" bordered=\"true\" open=\"true\">\n",
    "\n",
    "    <Panel value=\"Criteria 1\">\n",
    "      <View style=\"margin-top: 0.5em;\">\n",
    "        <Text name=\"criteria1_text\" value=\"$criteria1\" />\n",
    "        <Choices name=\"criteria1\" toName=\"criteria1_text\" choice=\"single\">\n",
    "          <Choice value=\"Criteria 1 is met\" />\n",
    "          <Choice value=\"Criteria 1 is not met\" />\n",
    "        </Choices>\n",
    "      </View>\n",
    "    </Panel>\n",
    "\n",
    "    <Panel value=\"Criteria 2\">\n",
    "      <View style=\"margin-top: 0.5em;\">\n",
    "        <Text name=\"criteria2_text\" value=\"$criteria2\" />\n",
    "        <Choices name=\"criteria2\" toName=\"criteria2_text\" choice=\"single\">\n",
    "          <Choice value=\"Criteria 2 is met\" />\n",
    "          <Choice value=\"Criteria 2 is not met\" />\n",
    "        </Choices>\n",
    "      </View>\n",
    "    </Panel>\n",
    "\n",
    "    <Panel value=\"Criteria 3\">\n",
    "      <View style=\"margin-top: 0.5em;\">\n",
    "        <Text name=\"criteria3_text\" value=\"$criteria3\" />\n",
    "        <Choices name=\"criteria3\" toName=\"criteria3_text\" choice=\"single\">\n",
    "          <Choice value=\"Criteria 3 is met\" />\n",
    "          <Choice value=\"Criteria 3 is not met\" />\n",
    "        </Choices>\n",
    "      </View>\n",
    "    </Panel>\n",
    "\n",
    "  </Collapse>\n",
    "</View>\n",
    "\"\"\"\n",
    "proj = ls.projects.create(\n",
    "    title=f\"AskAI Rubric Evaluation {date.today()}\",\n",
    "    label_config=label_config, \n",
    "    color=\"#00FFFF\",\n",
    "    workspace=target_workspace_id\n",
    ")\n",
    "\n",
    "rubric_evaluation_project_id = proj.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c674a2",
   "metadata": {},
   "source": [
    "### Step 4: Parse the rubrics we created in LS and upload them with their task data to the new rubric evaluation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell enables us to \"Martha Stewart\" this project with predefined rubrics \n",
    "# for ones we didn't create in LS\n",
    "import csv \n",
    "with open(\"Custom_Benchmarks_Full_Dataset.csv\", newline='') as f:\n",
    "    reader= csv.DictReader(f)\n",
    "    question_dict = {}\n",
    "    for row in reader: \n",
    "        question_dict[row[\"question_id\"]] = row\n",
    "display(question_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e68ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tasks = ls.tasks.list(project=rubric_creation_project_id)\n",
    "\n",
    "all_rubrics = {}\n",
    "for i, task in enumerate(tasks):\n",
    "    question_id = str(task.data[\"question_id\"])\n",
    "    rubric_items = {}\n",
    "\n",
    "    # Check if task has annotations before accessing\n",
    "    # If no annotations, we use the predefined rubrics\n",
    "    if not task.annotations or len(task.annotations) == 0:\n",
    "        for i in range(3): \n",
    "            rubric_items.update({f\"criteria{i+1}\": question_dict[question_id][f\"criteria{i+1}\"]})\n",
    "\n",
    "    # If there are annotations, we use the annotations\n",
    "    elif task.annotations and len(task.annotations) > 0:\n",
    "        for c, r in enumerate(task.annotations[0][\"result\"]):\n",
    "            if r[\"type\"] == \"chatmessage\":\n",
    "                # we don't want to duplicate tasks, so we only take the instance of \n",
    "                # the rubric with the number, since it includes the criteria\n",
    "                continue\n",
    "            if r[\"type\"] == \"number\":\n",
    "                rubric_items.update({f\"criteria{c+1}\": f\"{r['value']['chatmessage']['content']} \\n (Points: {r['value']['number']})\"})\n",
    "        \n",
    "    all_rubrics[question_id] = rubric_items\n",
    "display(all_rubrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f721fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id, criteria in rubrics.items():\n",
    "    print(task_id)\n",
    "    task_data = question_dict[str(task_id)]\n",
    "    for model in [\"chatgpt\", \"claude\"]:\n",
    "        criteria[\"question_id\"] = task_id\n",
    "        criteria[\"User_Query\"] = task_data[\"question\"]\n",
    "        criteria[\"AI_Response\"] = task_data[f\"response_{model}\"]\n",
    "        criteria[\"model\"] = model\n",
    "        \n",
    "        print(criteria)\n",
    "        for c in [\"criteria1\", \"criteria2\", \"criteria3\"]:\n",
    "            if not c in criteria.keys():\n",
    "                criteria[c] = task_data[c]\n",
    "                \n",
    "        ls.tasks.create(\n",
    "                project=rubric_evaluation_project_id,\n",
    "                data=criteria\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f742e9",
   "metadata": {},
   "source": [
    "### Step 5: Score the Rubrics with LLM Judges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d78aed",
   "metadata": {},
   "source": [
    "#### Use this Prompt: \n",
    "\n",
    "```\n",
    "You will be provided a query from a user and a potential response, as well as a series of criteria. For each criterion, your job is to decide whether the criteria is met or unmet. Check the appropriate box for each critiera. \n",
    "\n",
    "User Query: {User_Query}\n",
    "Response to Grade: {AI_Response}\n",
    "\n",
    "Criteria 1: {criteria1}\n",
    "\n",
    "Criteria 2: {criteria2}\n",
    "\n",
    "Criteria 3: {criteria3}\n",
    "```\n",
    "\n",
    "We'll run this prompt with two different LLM Judges in Prompts in Label Studio!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fca586",
   "metadata": {},
   "source": [
    "### Step 6: Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08226eaa",
   "metadata": {},
   "source": [
    "#### A: Get all tasks from LS and calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09fc76f",
   "metadata": {},
   "outputs": [],
   "source": [

    "tasks = ls.tasks.list(project=rubric_evaluation_project_id)\n",
    "total_scores = {}\n",
    "possible_scores = {}\n",
    "for task in tasks: \n",
    "    question_id = str(task.data[\"question_id\"])\n",
    "    model_id = task.data[\"model\"]\n",
    "\n",
    "    # First, we extract the point values from the rubric items for the task\n",
    "    points_dict = {}\n",
    "    possible_points = 0\n",
    "    for c in [\"criteria1\", \"criteria2\", \"criteria3\"]:\n",
    "        criteria, points = task.data[c].split(\"Points:\")\n",
    "        points = float(points.strip()[:-1])\n",
    "        possible_points += points if points > 0 else 0\n",
    "        points_dict[c] = points\n",
    "        \n",
    "    possible_scores[question_id] = possible_points\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    # If a human had to review to break a discrepancy, it'll show up in [annotations]\n",
    "    # We start there to collect the evaluations\n",
    "    if task.annotations:\n",
    "        for r in task.annotations[0][\"result\"]:\n",
    "            # If criteria is not met, we don't care about the score, they don't gain any points\n",
    "            if \"not met\" in r[\"value\"][\"choices\"][0]:\n",
    "                continue\n",
    "\n",
    "            # If criteria is met, we add the points to the total score\n",
    "            else: \n",
    "                total += points_dict[r[\"from_name\"]]\n",
    "\n",
    "    # If the models agreed, we didn't use human review\n",
    "    # These show up in predictions, but since they're the same, we don't care which model they come from.\n",
    "    else: \n",
    "        for r in task.predictions[0][\"result\"]:\n",
    "             # If criteria is not met, we don't care about the score, they don't gain any points\n",
    "            if \"not met\" in r[\"value\"][\"choices\"][0]:\n",
    "                continue\n",
    "\n",
    "            # If criteria is met, we add the points to the total score\n",
    "            else: \n",
    "                total += points_dict[r[\"from_name\"]]\n",
    "\n",
    "    total_scores[(question_id, model_id)] = total\n",
    "\n",
    "display(total_scores)\n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995ec69",
   "metadata": {},
   "source": [
    "#### B: Analytics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_possible_points = sum(possible_scores.values())\n",
    "final_scores = {'chatgpt': 0, 'claude': 0}\n",
    "percentages_chatgpt = []\n",
    "percentages_claude = []\n",
    "for (question_id, model_id), score in total_scores.items():\n",
    "    final_scores[str(model_id)] += score\n",
    "    if model_id == \"chatgpt\":\n",
    "        percentages_chatgpt.append(score / possible_scores[question_id])\n",
    "    elif model_id == \"claude\":\n",
    "        percentages_claude.append(score / possible_scores[question_id])\n",
    "        \n",
    "final_scores[\"chatgpt\"] = final_scores[\"chatgpt\"] / total_possible_points\n",
    "final_scores[\"claude\"] = final_scores[\"claude\"] / total_possible_points\n",
    "\n",
    "display(final_scores)\n",
    "\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8738076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define bins for the distribution (0 to 1.0, divided into 10 bins)\n",
    "bins = np.linspace(0, 1.0, 11)  # Creates bins: [0, 0.1, 0.2, ..., 1.0]\n",
    "\n",
    "# Calculate histograms for both distributions\n",
    "hist_chatgpt, bin_edges = np.histogram(percentages_chatgpt, bins=bins)\n",
    "hist_claude, _ = np.histogram(percentages_claude, bins=bins)\n",
    "\n",
    "# Create bin centers for x-axis labels\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "# Create the bar chart\n",
    "x = np.arange(len(bin_centers))\n",
    "width = bin_width * 0.35  # Width of bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create bars for each model\n",
    "bars1 = ax.bar(x - width/2, hist_chatgpt, width, label='ChatGPT', alpha=0.8, color='#4A90E2')\n",
    "bars2 = ax.bar(x + width/2, hist_claude, width, label='Claude', alpha=0.8, color='#F5A623')\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel('Score Range (Percentage)', fontsize=12)\n",
    "ax.set_ylabel('Frequency (Number of Questions)', fontsize=12)\n",
    "ax.set_title('Distribution of Scores: ChatGPT vs Claude', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{bin_edges[i]:.1f}-{bin_edges[i+1]:.1f}' for i in range(len(bin_edges)-1)], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c01ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define bins for the distribution (0 to 1.0, divided into 10 bins)\n",
    "bins = np.linspace(0, 1.0, 11)  # Creates bins: [0, 0.1, 0.2, ..., 1.0]\n",
    "\n",
    "# Calculate histograms for both distributions\n",
    "hist_chatgpt, bin_edges = np.histogram(percentages_chatgpt, bins=bins)\n",
    "hist_claude, _ = np.histogram(percentages_claude, bins=bins)\n",
    "\n",
    "# Create bin centers for x-axis labels\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "# Create the bar chart\n",
    "x = np.arange(len(bin_centers))\n",
    "width = bin_width * 0.35  # Width of bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create bars for each model\n",
    "bars1 = ax.bar(x - width/2, hist_chatgpt, width, label='ChatGPT', alpha=0.8, color='#4A90E2')\n",
    "bars2 = ax.bar(x + width/2, hist_claude, width, label='Claude', alpha=0.8, color='#F5A623')\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel('Score Range (Percentage)', fontsize=12)\n",
    "ax.set_ylabel('Frequency (Number of Questions)', fontsize=12)\n",
    "ax.set_title('Distribution of Scores: ChatGPT vs Claude', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{bin_edges[i]:.1f}-{bin_edges[i+1]:.1f}' for i in range(len(bin_edges)-1)], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c01ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
